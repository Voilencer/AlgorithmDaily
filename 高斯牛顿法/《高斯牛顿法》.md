[TOC]

非线性优化算法——高斯牛顿法

***

# 概述

* Gauss-Newton
* 非线性最小二乘优化
* 针对牛顿法中的Hessian矩阵计算复杂，对其近似简化

# 推论

* 目标函数优化，转化为最小化误差`f(x)`平方和

$$
minF(x_0+\triangle x) = ||f(x_0+\triangle x||^2
$$

* 一阶泰勒展开

$$
minF(x_0+\triangle x) = ||f(x_0)+J^T\triangle x||^2
$$

* 雅可比矩阵`J`

$$
J=\left[\begin{matrix}
\frac{∂y_1}{x_1} ... \frac{∂y_1}{x_n} \\
\cdots \\
\frac{∂y_m}{x_1} ... \frac{∂y_m}{x_n} \\
\end{matrix} \right]
$$

* 求极值，函数求导，导数为0

$$
\frac{∂||f(x_0)+J^T\triangle x||^2}{∂\triangle x} = 0
$$



* 求解得到

$$
JJ^T\triangle x = -Jf(x_0)
$$

* 迭代公式为

$$
\triangle x = -(J^TJ)^{-1}J^Tf(x_0)
$$

# 应用

* 仿真曲线

```
x = np.random.randn(20, 1)
mean = 0
var = 0.5
noise = np.random.normal(0, 1 ** 0.5, (20, 1))
y = 3 * x**2 + 4 * x + noise # 高斯噪声
```

* 高斯牛顿曲线拟合

```
def func(x, para):
    return para[0][0] * x**2 + para[1][0] * x

def jacobi(x, para):
    J = np.zeros((len(x), para.shape[0]))
    for i, item in enumerate(x):
        J[i, 0] = item ** 2
        J[i, 1] = item
    return np.matrix(J)

def fit_curve(x, y, iter_num = 10000):
    para = np.ones((2,1))  # (1) 参数初始化
    for i in range(iter_num):
        y_gj = func(x, para)
        r = np.matrix(y - y_gj) # (2) 误差
        J = jacobi(x, para)
        tmp = (J.T * J).I * J.T *  r # (3) 迭代
        para_ = para + tmp.A # (4) 参数更新
        if all(abs(tmp.A)) < 1e-6: # （5）迭代终止
            break
        para = para_
    return para
```

* 可视化

<img src="C:\Users\Voilencer\AppData\Roaming\Typora\typora-user-images\image-20210817222345233.png" alt="image-20210817222345233" style="zoom: 67%;" />